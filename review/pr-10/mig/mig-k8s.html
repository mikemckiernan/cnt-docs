<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>MIG Support in Kubernetes &mdash; NVIDIA Cloud Native Technologies  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/nvidia.ico"/>
    <link rel="canonical" href="https://docs.nvidia.com/datacenter/cloud-native/mig/mig-k8s.html"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/js/google-analytics/google-analytics-tracker.js"></script>
        <script src="../_static/js/google-analytics/google-analytics-write.js"></script>
        <script src="//assets.adobedtm.com/b92787824f2e0e9b68dc2e993f9bd995339fe417/satelliteLib-7ba51e58dc61bcb0e9311aadd02a0108ab24cc6c.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" > 
            <a href="../contents.html">
            <img src="../_static/NVLogo_H_B&W.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }

    .wy-nav-content {
      max-width: 1000px;
    }
  </style>
  
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">GPU Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/about.html">About the Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/platform-support.html">Platform Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/getting-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/getting-started.html#install-nvidia-gpu-operator">Install NVIDIA GPU Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/gpu-driver-upgrades.html">GPU Driver Upgrades</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/advanced-configurations.html">Advanced Configurations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/openshift/contents.html">Red Hat OpenShift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/appendix.html">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/license.html">Licenses and Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/archive.html">Archive</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../contents.html">NVIDIA Cloud Native Technologies</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../contents.html" class="icon icon-home"></a> &raquo;</li>
      <li>MIG Support in Kubernetes</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="mig-support-in-kubernetes">
<span id="mig-kubernetes"></span><h1>MIG Support in Kubernetes<a class="headerlink" href="#mig-support-in-kubernetes" title="Permalink to this heading"></a></h1>
<p>This document provides steps on getting started and running some example CUDA workloads
on MIG-enabled GPUs in a Kubernetes cluster.</p>
<section id="software-pre-requisites">
<h2>Software Pre-requisites<a class="headerlink" href="#software-pre-requisites" title="Permalink to this heading"></a></h2>
<p>The deployment workflow requires these prerequisites. Once these prerequisites have been met,
you can proceed to deploy a MIG capable version of the NVIDIA <code class="docutils literal notranslate"><span class="pre">k8s-device-plugin</span></code> and
the <code class="docutils literal notranslate"><span class="pre">gpu-feature-discovery</span></code> component in your cluster, so that Kubernetes can schedule
pods on the available MIG devices.</p>
<ol class="arabic">
<li><p>You already have a Kubernetes deployment up and running with access to at least one NVIDIA A100 GPU.</p></li>
<li><p>The node with the NVIDIA A100 GPU is running the following versions of NVIDIA software:</p>
<blockquote>
<div><ul class="simple">
<li><p>NVIDIA <a class="reference external" href="https://docs.nvidia.com/datacenter/tesla/drivers/index.html">datacenter driver</a> &gt;= 450.80.02</p></li>
<li><p>NVIDIA Container Toolkit (<code class="docutils literal notranslate"><span class="pre">nvidia-docker2</span></code>) &gt;= 2.5.0 (and corresponding <code class="docutils literal notranslate"><span class="pre">libnvidia-container</span></code> &gt;= 1.3.3)</p></li>
</ul>
</div></blockquote>
</li>
<li><p>NVIDIA <a class="reference external" href="https://github.com/NVIDIA/k8s-device-plugin/tree/v0.7.0">k8s-device-plugin</a>: v0.7.0+</p></li>
<li><p>NVIDIA <a class="reference external" href="https://github.com/NVIDIA/gpu-feature-discovery/tree/v0.2.0">gpu-feature-discovery</a>: v0.2.0+</p></li>
</ol>
</section>
<section id="getting-started">
<h2>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this heading"></a></h2>
<section id="install-kubernetes">
<h3>Install Kubernetes<a class="headerlink" href="#install-kubernetes" title="Permalink to this heading"></a></h3>
<p>As a first step, ensure that you have a Kubernetes deployment set up with a control plane and nodes joined to the
cluster. Follow the <a class="reference internal" href="../kubernetes/install-k8s.html#install-k8s"><span class="std std-ref">Install Kubernetes</span></a> guide for getting started with setting up a Kubernetes cluster.</p>
</section>
<section id="configuration-strategy">
<h3>Configuration Strategy<a class="headerlink" href="#configuration-strategy" title="Permalink to this heading"></a></h3>
<p>TBD.</p>
</section>
<section id="setting-up-mig-geometry">
<h3>Setting up MIG Geometry<a class="headerlink" href="#setting-up-mig-geometry" title="Permalink to this heading"></a></h3>
<p>You can either use NVML (or its command-line interface <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code>) to configure the desired MIG geometry. For automation,
we recommend using tooling such as <a class="reference external" href="https://github.com/nvidia/mig-parted">mig-parted</a> that allows configuring MIG mode
and creating the desired profiles on the GPUs.</p>
<p>In this step, let’s use <code class="docutils literal notranslate"><span class="pre">mig-parted</span></code> to configure the A100 into 7 GPUs (using the <code class="docutils literal notranslate"><span class="pre">1g.5gb</span></code> profile):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>nvidia-mig-parted<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>config.yaml<span class="w"> </span>-c<span class="w"> </span>all-1g.5gb
</pre></div>
</div>
<p>Now, the A100 should be configured into 7 MIG devices:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>nvidia-smi<span class="w"> </span>mig<span class="w"> </span>-lgi
<span class="go">+----------------------------------------------------+</span>
<span class="go">| GPU instances:                                     |</span>
<span class="go">| GPU   Name          Profile  Instance   Placement  |</span>
<span class="go">|                       ID       ID       Start:Size |</span>
<span class="go">|====================================================|</span>
<span class="go">|   0  MIG 1g.5gb       19        7          0:1     |</span>
<span class="go">+----------------------------------------------------+</span>
<span class="go">|   0  MIG 1g.5gb       19        8          1:1     |</span>
<span class="go">+----------------------------------------------------+</span>
<span class="go">|   0  MIG 1g.5gb       19        9          2:1     |</span>
<span class="go">+----------------------------------------------------+</span>
<span class="go">|   0  MIG 1g.5gb       19       10          3:1     |</span>
<span class="go">+----------------------------------------------------+</span>
<span class="go">|   0  MIG 1g.5gb       19       11          4:1     |</span>
<span class="go">+----------------------------------------------------+</span>
<span class="go">|   0  MIG 1g.5gb       19       12          5:1     |</span>
<span class="go">+----------------------------------------------------+</span>
<span class="go">|   0  MIG 1g.5gb       19       13          6:1     |</span>
<span class="go">+----------------------------------------------------+</span>
</pre></div>
</div>
</section>
<section id="deploying-the-nvidia-device-plugin-and-gfd">
<h3>Deploying the NVIDIA Device Plugin and GFD<a class="headerlink" href="#deploying-the-nvidia-device-plugin-and-gfd" title="Permalink to this heading"></a></h3>
<section id="nvidia-device-plugin">
<h4>NVIDIA Device Plugin<a class="headerlink" href="#nvidia-device-plugin" title="Permalink to this heading"></a></h4>
<p>Depending on the MIG configuration strategy used for the cluster, deploy the NVIDIA device plugin with the right options.
In this example, we assume that the user has chosen a <code class="docutils literal notranslate"><span class="pre">single</span></code> MIG strategy for the cluster.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--set<span class="w"> </span><span class="nv">migStrategy</span><span class="o">=</span>single<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>nvdp/nvidia-device-plugin
</pre></div>
</div>
<p>At this point, the <cite>nvidia-device-plugin</cite> daemonset should be deployed and enumerated the MIG devices to Kubernetes:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">2021/04/26 23:19:15 Loading NVML</span>
<span class="go">2021/04/26 23:19:15 Starting FS watcher.</span>
<span class="go">2021/04/26 23:19:15 Starting OS watcher.</span>
<span class="go">2021/04/26 23:19:15 Retreiving plugins.</span>
<span class="go">2021/04/26 23:19:16 Starting GRPC server for &#39;nvidia.com/gpu&#39;</span>
<span class="go">2021/04/26 23:19:16 Starting to serve &#39;nvidia.com/gpu&#39; on /var/lib/kubelet/device-plugins/nvidia-gpu.sock</span>
<span class="go">2021/04/26 23:19:16 Registered device plugin for &#39;nvidia.com/gpu&#39; with Kubelet</span>
</pre></div>
</div>
</section>
<section id="id1">
<h4>GPU Feature Discovery<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h4>
<p>Next, we deploy the <a class="reference external" href="https://github.com/NVIDIA/gpu-feature-discovery">GPU Feature Discovery (GFD)</a> plugin to label the GPU nodes
so that users can specific MIG devices as resources in their podspec. Note that the GFD Helm chart also deploys the Node Feature Discovery
(NFD) as a prerequisite:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>repo<span class="w"> </span>add<span class="w"> </span>nvgfd<span class="w"> </span>https://nvidia.github.io/gpu-feature-discovery<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">&amp;&amp;</span><span class="w"> </span>helm<span class="w"> </span>repo<span class="w"> </span>update
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--set<span class="w"> </span><span class="nv">migStrategy</span><span class="o">=</span>single<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>nvgfd/gpu-feature-discovery
</pre></div>
</div>
<p>At this point, we can verify that all pods are running:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">NAMESPACE                NAME                                       READY   STATUS    RESTARTS   AGE</span>
<span class="go">kube-system              calico-kube-controllers-6d8ccdbf46-wjst8   1/1     Running   1          4h58m</span>
<span class="go">kube-system              calico-node-qp5wf                          1/1     Running   1          4h58m</span>
<span class="go">kube-system              coredns-558bd4d5db-c6nhk                   1/1     Running   1          4h59m</span>
<span class="go">kube-system              coredns-558bd4d5db-cgjr7                   1/1     Running   1          4h59m</span>
<span class="go">kube-system              etcd-ipp1-0552                             1/1     Running   1          5h</span>
<span class="go">kube-system              kube-apiserver-ipp1-0552                   1/1     Running   1          5h</span>
<span class="go">kube-system              kube-controller-manager-ipp1-0552          1/1     Running   1          5h</span>
<span class="go">kube-system              kube-proxy-d7tqd                           1/1     Running   1          4h59m</span>
<span class="go">kube-system              kube-scheduler-ipp1-0552                   1/1     Running   1          5h</span>
<span class="go">kube-system              nvidia-device-plugin-1619479152-646qm      1/1     Running   0          115m</span>
<span class="go">node-feature-discovery   gpu-feature-discovery-1619479450-f7rvv     1/1     Running   0          110m</span>
<span class="go">node-feature-discovery   nfd-master-74f76f6c68-zgt9d                1/1     Running   0          110m</span>
<span class="go">node-feature-discovery   nfd-worker-pkdn2                           1/1     Running   0          110m</span>
</pre></div>
</div>
<p>And the node has been labeled:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>node<span class="w"> </span>-o<span class="w"> </span>json<span class="w"> </span><span class="p">|</span><span class="w"> </span>jq<span class="w"> </span><span class="s1">&#39;.items[].metadata.labels&#39;</span>
</pre></div>
</div>
<p>with labels:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span>...
&quot;node-role.kubernetes.io/master&quot;: &quot;&quot;,
&quot;node.kubernetes.io/exclude-from-external-load-balancers&quot;: &quot;&quot;,
&quot;nvidia.com/cuda.driver.major&quot;: &quot;460&quot;,
&quot;nvidia.com/cuda.driver.minor&quot;: &quot;73&quot;,
&quot;nvidia.com/cuda.driver.rev&quot;: &quot;01&quot;,
&quot;nvidia.com/cuda.runtime.major&quot;: &quot;11&quot;,
&quot;nvidia.com/cuda.runtime.minor&quot;: &quot;2&quot;,
&quot;nvidia.com/gfd.timestamp&quot;: &quot;1619479472&quot;,
&quot;nvidia.com/gpu.compute.major&quot;: &quot;8&quot;,
&quot;nvidia.com/gpu.compute.minor&quot;: &quot;0&quot;,
&quot;nvidia.com/gpu.count&quot;: &quot;7&quot;,
&quot;nvidia.com/gpu.engines.copy&quot;: &quot;1&quot;,
&quot;nvidia.com/gpu.engines.decoder&quot;: &quot;0&quot;,
&quot;nvidia.com/gpu.engines.encoder&quot;: &quot;0&quot;,
&quot;nvidia.com/gpu.engines.jpeg&quot;: &quot;0&quot;,
&quot;nvidia.com/gpu.engines.ofa&quot;: &quot;0&quot;,
&quot;nvidia.com/gpu.family&quot;: &quot;ampere&quot;,
&quot;nvidia.com/gpu.machine&quot;: &quot;SYS-1019GP-TT-02-NC24B&quot;,
&quot;nvidia.com/gpu.memory&quot;: &quot;4864&quot;,
&quot;nvidia.com/gpu.multiprocessors&quot;: &quot;14&quot;,
&quot;nvidia.com/gpu.product&quot;: &quot;A100-PCIE-40GB-MIG-1g.5gb&quot;,
&quot;nvidia.com/gpu.slices.ci&quot;: &quot;1&quot;,
&quot;nvidia.com/gpu.slices.gi&quot;: &quot;1&quot;,
&quot;nvidia.com/mig.strategy&quot;: &quot;single&quot;
}
</pre></div>
</div>
<p>We can now proceed to run some sample workloads.</p>
</section>
</section>
<section id="running-sample-cuda-workloads">
<span id="mig-examples"></span><h3>Running Sample CUDA Workloads<a class="headerlink" href="#running-sample-cuda-workloads" title="Permalink to this heading"></a></h3>
<section id="cuda-vectoradd">
<h4>CUDA VectorAdd<a class="headerlink" href="#cuda-vectoradd" title="Permalink to this heading"></a></h4>
<p>Let’s run a simple CUDA sample, in this case <code class="docutils literal notranslate"><span class="pre">vectorAdd</span></code> by requesting a GPU resource as you would
normally do in Kubernetes. In this case, Kubernetes will schedule the pod on a single MIG device and
we use a <code class="docutils literal notranslate"><span class="pre">nodeSelector</span></code> to direct the pod to be scheduled on the node with the MIG devices.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat<span class="w"> </span>&lt;&lt;<span class="w"> </span>EOF<span class="w"> </span><span class="p">|</span><span class="w"> </span>kubectl<span class="w"> </span>create<span class="w"> </span>-f<span class="w"> </span>-
<span class="go">apiVersion: v1</span>
<span class="go">kind: Pod</span>
<span class="go">metadata:</span>
<span class="go">  name: cuda-vectoradd</span>
<span class="go">spec:</span>
<span class="go">  restartPolicy: OnFailure</span>
<span class="go">  containers:</span>
<span class="go">  - name: vectoradd</span>
<span class="go">    image: nvidia/samples:vectoradd-cuda11.2.1</span>
<span class="go">    resources:</span>
<span class="go">      limits:</span>
<span class="go">        nvidia.com/gpu: 1</span>
<span class="go">  nodeSelector:</span>
<span class="go">    nvidia.com/gpu.product: A100-SXM4-40GB-MIG-1g.5gb</span>
<span class="go">EOF</span>
</pre></div>
</div>
</section>
<section id="concurrent-job-launch">
<h4>Concurrent Job Launch<a class="headerlink" href="#concurrent-job-launch" title="Permalink to this heading"></a></h4>
<p>Now, let’s try a more complex example. In this example, we will use Argo Workflows to launch concurrent
jobs on MIG devices. In this example, the A100 has been configured into 2 MIG devices using the: <code class="docutils literal notranslate"><span class="pre">3g.20gb</span></code> profile.</p>
<p>First, <a class="reference external" href="https://argoproj.github.io/argo-workflows/quick-start/#install-argo-workflows">install</a> the Argo Workflows
components into your Kubernetes cluster.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>create<span class="w"> </span>ns<span class="w"> </span>argo<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">&amp;&amp;</span><span class="w"> </span>kubectl<span class="w"> </span>apply<span class="w"> </span>-n<span class="w"> </span>argo<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-f<span class="w"> </span>https://raw.githubusercontent.com/argoproj/argo-workflows/stable/manifests/quick-start-postgres.yaml
</pre></div>
</div>
<p>Next, download the latest Argo CLI from the <a class="reference external" href="https://github.com/argoproj/argo-workflows/releases">releases page</a> and
follow the instructions to install the binary.</p>
<p>Now, we will craft an Argo example that launches multiple CUDA containers onto the MIG devices on the GPU.
We will reuse the same <code class="docutils literal notranslate"><span class="pre">vectorAdd</span></code> example from before. Here is the job description, saved as <code class="docutils literal notranslate"><span class="pre">vector-add.yaml</span></code>:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">$ cat &lt;&lt; EOF &gt; vector-add.yaml</span>
<span class="l l-Scalar l-Scalar-Plain">apiVersion</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">argoproj.io/v1alpha1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Workflow</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="nt">generateName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">argo-mig-example-</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="nt">entrypoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">argo-mig-result-example</span>
<span class="nt">templates</span><span class="p">:</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">argo-mig-result-example</span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">steps</span><span class="p p-Indicator">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">generate</span>
<span class="w">        </span><span class="nt">template</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gen-mig-device-list</span>
<span class="w">    </span><span class="c1"># Iterate over the list of numbers generated by the generate step above</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">argo-mig</span>
<span class="w">        </span><span class="nt">template</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">argo-mig</span>
<span class="w">        </span><span class="nt">arguments</span><span class="p">:</span>
<span class="w">        </span><span class="nt">parameters</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">argo-mig</span>
<span class="w">            </span><span class="l l-Scalar l-Scalar-Plain">value</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;{{item}}&quot;</span>
<span class="w">        </span><span class="nt">withParam</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;{{steps.generate.outputs.result}}&quot;</span>

<span class="c1"># Generate a list of numbers in JSON format</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gen-mig-device-list</span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">script</span><span class="p p-Indicator">:</span>
<span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python:alpine3.6</span>
<span class="w">    </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">python</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="nt">source</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">        </span><span class="no">import json</span>
<span class="w">        </span><span class="no">import sys</span>
<span class="w">        </span><span class="no">json.dump([i for i in range(0, 2)], sys.stdout)</span>

<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">argo-mig</span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">retryStrategy</span><span class="p p-Indicator">:</span>
<span class="w">    </span><span class="nt">limit</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
<span class="w">    </span><span class="nt">retryPolicy</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Always&quot;</span>
<span class="w">    </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">    </span><span class="nt">parameters</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">argo-mig</span>
<span class="w">    </span><span class="nt">container</span><span class="p">:</span>
<span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia/samples:vectoradd-cuda11.2.1</span>
<span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<span class="w">        </span><span class="nt">limits</span><span class="p">:</span>
<span class="w">        </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">nodeSelector</span><span class="p">:</span>
<span class="w">    </span><span class="nt">nvidia.com/gpu.product</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">A100-SXM4-40GB-MIG-3g.20gb</span>
<span class="l l-Scalar l-Scalar-Plain">EOF</span>
</pre></div>
</div>
<p>Launch the workflow:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>argo<span class="w"> </span>submit<span class="w"> </span>-n<span class="w"> </span>argo<span class="w"> </span>--watch<span class="w"> </span>vector-add.yaml
</pre></div>
</div>
<p>Argo will print out the pods that have been launched:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">Name:                argo-mig-example-z6mqd</span>
<span class="go">Namespace:           argo</span>
<span class="go">ServiceAccount:      default</span>
<span class="go">Status:              Succeeded</span>
<span class="go">Conditions:</span>
<span class="go">Completed           True</span>
<span class="go">Created:             Wed Mar 24 14:44:51 -0700 (20 seconds ago)</span>
<span class="go">Started:             Wed Mar 24 14:44:51 -0700 (20 seconds ago)</span>
<span class="go">Finished:            Wed Mar 24 14:45:11 -0700 (now)</span>
<span class="go">Duration:            20 seconds</span>
<span class="go">Progress:            3/3</span>
<span class="go">ResourcesDuration:   9s*(1 cpu),9s*(100Mi memory),1s*(1 nvidia.com/gpu)</span>

<span class="go">STEP                       TEMPLATE                 PODNAME                           DURATION  MESSAGE</span>
<span class="go">✔ argo-mig-example-z6mqd  argo-mig-result-example</span>
<span class="go">├───✔ generate            gen-mig-device-list      argo-mig-example-z6mqd-562792713  8s</span>
<span class="go">└─┬─✔ argo-mig(0:0)(0)    argo-mig                 argo-mig-example-z6mqd-845918106  2s</span>
<span class="go">└─✔ argo-mig(1:1)(0)    argo-mig                 argo-mig-example-z6mqd-870679174  2s</span>
</pre></div>
</div>
<p>If you observe the logs, you can see that the <code class="docutils literal notranslate"><span class="pre">vector-add</span></code> sample has completed on both devices:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>argo<span class="w"> </span>logs<span class="w"> </span>-n<span class="w"> </span>argo<span class="w"> </span>@latest
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">argo-mig-example-z6mqd-562792713: [0, 1]</span>
<span class="go">argo-mig-example-z6mqd-870679174: [Vector addition of 50000 elements]</span>
<span class="go">argo-mig-example-z6mqd-870679174: Copy input data from the host memory to the CUDA device</span>
<span class="go">argo-mig-example-z6mqd-870679174: CUDA kernel launch with 196 blocks of 256 threads</span>
<span class="go">argo-mig-example-z6mqd-870679174: Copy output data from the CUDA device to the host memory</span>
<span class="go">argo-mig-example-z6mqd-870679174: Test PASSED</span>
<span class="go">argo-mig-example-z6mqd-870679174: Done</span>
<span class="go">argo-mig-example-z6mqd-845918106: [Vector addition of 50000 elements]</span>
<span class="go">argo-mig-example-z6mqd-845918106: Copy input data from the host memory to the CUDA device</span>
<span class="go">argo-mig-example-z6mqd-845918106: CUDA kernel launch with 196 blocks of 256 threads</span>
<span class="go">argo-mig-example-z6mqd-845918106: Copy output data from the CUDA device to the host memory</span>
<span class="go">argo-mig-example-z6mqd-845918106: Test PASSED</span>
<span class="go">argo-mig-example-z6mqd-845918106: Done</span>
</pre></div>
</div>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018-2023, NVIDIA Corporation.
      <span class="lastupdated">Last updated on 2023-03-14.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>  

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>