<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>GPU Driver Upgrades &mdash; gpu-operator 23.0.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/api-styles.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/mermaid-init.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/design-tabs.js"></script>
        <script src="_static/version.js"></script>
        <script src="_static/social-media.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="NVIDIA vGPU" href="install-gpu-operator-vgpu.html" />
    <link rel="prev" title="Getting Started" href="getting-started.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
          </a>

<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">NVIDIA GPU Operator</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="about.html">About the Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="platform-support.html">Platform Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">GPU Driver Upgrades</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#upgrades-with-the-upgrade-controller">Upgrades with the Upgrade Controller</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#configuration-options">Configuration options</a></li>
<li class="toctree-l3"><a class="reference internal" href="#upgrade-state-machine">Upgrade State Machine</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pausing-driver-upgrades">Pausing Driver Upgrades</a></li>
<li class="toctree-l3"><a class="reference internal" href="#skipping-driver-upgrades">Skipping Driver Upgrades</a></li>
<li class="toctree-l3"><a class="reference internal" href="#metrics-and-events">Metrics and Events</a></li>
<li class="toctree-l3"><a class="reference internal" href="#troubleshooting">Troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#upgrades-without-the-upgrade-controller">Upgrades without the Upgrade Controller</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">Configuration Options</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-vgpu.html">NVIDIA vGPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-nvaie.html">NVIDIA AI Enterprise</a></li>
<li class="toctree-l1"><a class="reference internal" href="openshift/contents.html">GPU Operator on OpenShift</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-mig.html">GPU Operator with MIG</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-sharing.html">Time-Slicing GPUs in Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-rdma.html">GPUDirect RDMA and GPUDirect Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-kubevirt.html">GPU Operator with KubeVirt</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix.html">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">Licenses and Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="archive.html">Archive</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Related Projects</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html">NVIDIA Container Toolkit</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/kubernetes/install-k8s.html">Kubernetes with GPUs</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-telemetry/dcgm-exporter.html">GPU Telemetry</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/mig/mig.html">Multi-Instance GPU</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/driver-containers/overview.html">Driver Containers</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">gpu-operator</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
<li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
<li>GPU Driver Upgrades</li>
<li class="wy-breadcrumbs-aside">

  <span>&nbsp;</span>
</li>

  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="gpu-driver-upgrades">
<span id="id1"></span><h1>GPU Driver Upgrades<a class="headerlink" href="#gpu-driver-upgrades" title="Permalink to this headline"></a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline"></a></h2>
<p>The NVIDIA driver daeamonset requires special consideration for upgrades because the driver kernel modules must be unloaded and loaded again on each driver container restart.
Consequently, the following steps must occur across a driver upgrade:</p>
<ol class="arabic simple">
<li><p>Disable all clients to the GPU driver.</p></li>
<li><p>Unload the current GPU driver kernel modules.</p></li>
<li><p>Start the updated GPU driver pod.</p></li>
<li><p>Install the updated GPU driver and load the updated kernel modules.</p></li>
<li><p>Enable the clients of the GPU driver.</p></li>
</ol>
<p>The GPU Operator supports several methods for managing and automating this driver upgrade process.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The GPU Operator only manages the lifecycle of containerized drivers.
Drivers which are pre-installed on the host are not managed by the GPU Operator.</p>
</div>
</section>
<section id="upgrades-with-the-upgrade-controller">
<h2>Upgrades with the Upgrade Controller<a class="headerlink" href="#upgrades-with-the-upgrade-controller" title="Permalink to this headline"></a></h2>
<p>NVIDIA recommends upgrading by using the upgrade controller and the controller is enabled by default in the GPU Operator.
The controller automates the upgrade process and generates metrics and events so that you can monitor the upgrade process.</p>
<p class="rubric">Procedure</p>
<ol class="arabic">
<li><p>Upgrade the driver by changing <code class="docutils literal notranslate"><span class="pre">driver.version</span></code> value in ClusterPolicy:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl patch clusterpolicy/cluster-policy --type<span class="o">=</span><span class="s1">&#39;json&#39;</span> -p<span class="o">=</span><span class="s1">&#39;[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/driver/version&quot;, &quot;value&quot;:&quot;510.85.02&quot;}]&#39;</span>
</pre></div>
</div>
</li>
<li><p>(Optional) For each node, monitor the upgrade status:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get node -l nvidia.com/gpu.present <span class="se">\</span>
   -ojsonpath<span class="o">=</span><span class="s1">&#39;{range .items[*]}{.metadata.name}{&quot;\t&quot;}{.metadata.labels.nvidia\.com/gpu-driver-upgrade-state}{&quot;\n&quot;}{end}&#39;</span>
</pre></div>
</div>
</li>
</ol>
<section id="configuration-options">
<h3>Configuration options<a class="headerlink" href="#configuration-options" title="Permalink to this headline"></a></h3>
<p>The following fields in ClusterPolicy are available to configure the upgrade controller:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">driver</span><span class="p">:</span><span class="w"></span>

<span class="w">  </span><span class="nt">upgradePolicy</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="c1"># autoUpgrade (default=true): Switch which enables / disables the driver upgrade controller.</span><span class="w"></span>
<span class="w">    </span><span class="c1">#     If set to false all other options are ignored.</span><span class="w"></span>
<span class="w">    </span><span class="nt">autoUpgrade</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">    </span><span class="c1"># maxParallelUpgrades (default=1): Number of nodes that can be upgraded in parallel. 0 means infinite.</span><span class="w"></span>
<span class="w">    </span><span class="nt">maxParallelUpgrades</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>

<span class="w">    </span><span class="c1"># waitForCompletion: Options for the &#39;wait-for-completion&#39; state, which will wait for a user-defined group of pods</span><span class="w"></span>
<span class="w">    </span><span class="c1">#    to complete before upgrading the driver on a node.</span><span class="w"></span>
<span class="w">    </span><span class="nt">waitForCompletion</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="c1"># timeoutSeconds (default=0): The length of time to wait before giving up. 0 means infinite.</span><span class="w"></span>
<span class="w">      </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span><span class="w"></span>
<span class="w">      </span><span class="c1"># podSelector (default=&quot;&quot;): The label selector defining the group of pods to wait for completion of. &quot;&quot; means to wait on none.</span><span class="w"></span>
<span class="w">      </span><span class="nt">podSelector</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="c1"># gpuPodDeletion: Options for the &#39;pod-deletion&#39; state, which will evict all pods on the node allocated a GPU.</span><span class="w"></span>
<span class="w">    </span><span class="nt">gpuPodDeletion</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="c1"># force (default=false): Delete pods even if they are not managed by a controller (e.g. ReplicationController, ReplicaSet,</span><span class="w"></span>
<span class="w">      </span><span class="c1">#    Job, DaemonSet or StatefulSet).</span><span class="w"></span>
<span class="w">      </span><span class="nt">force</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>
<span class="w">      </span><span class="c1"># timeoutSeconds (default=300): The length of time to wait before giving up. 0 means infinite. When the timeout is met,</span><span class="w"></span>
<span class="w">      </span><span class="c1">#    the GPU  pod(s) will be forcefully deleted.</span><span class="w"></span>
<span class="w">      </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">300</span><span class="w"></span>
<span class="w">      </span><span class="c1"># deleteEmptyDir (default=false): Delete pods even if they are using emptyDir volumes (local data will be deleted).</span><span class="w"></span>
<span class="w">      </span><span class="nt">deleteEmptyDir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>

<span class="w">    </span><span class="c1"># drain: Options for the &#39;drain&#39; state, which will drain the node (i.e. &#39;kubectl drain&#39;). This is only performed if</span><span class="w"></span>
<span class="w">    </span><span class="c1">#    enabled and the &#39;pod-deletion&#39; state cannot successfully remove all pods using GPU.</span><span class="w"></span>
<span class="w">    </span><span class="nt">drain</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="c1"># enable (default=false): Switch for allowing node drain during the upgrade process</span><span class="w"></span>
<span class="w">      </span><span class="nt">enable</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>
<span class="w">      </span><span class="c1"># force (default=false): Delete pods even if they are not managed by a controller (e.g. ReplicationController, ReplicaSet,</span><span class="w"></span>
<span class="w">      </span><span class="c1">#    Job, DaemonSet or StatefulSet).</span><span class="w"></span>
<span class="w">      </span><span class="nt">force</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>
<span class="w">      </span><span class="c1"># podSelector (default=&quot;&quot;): The label selector to filter pods on the node. &quot;&quot; will drain all pods.</span><span class="w"></span>
<span class="w">      </span><span class="nt">podSelector</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"></span>
<span class="w">      </span><span class="c1"># timeoutSeconds (default=300): The length of time to wait before giving up. 0 means infinite. When the timeout is met,</span><span class="w"></span>
<span class="w">      </span><span class="c1">#    the GPU  pod(s) will be forcefully deleted.</span><span class="w"></span>
<span class="w">      </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">300</span><span class="w"></span>
<span class="w">      </span><span class="c1"># deleteEmptyDir (default=false): Delete pods even if they are using emptyDir volumes (local data will be deleted).</span><span class="w"></span>
<span class="w">      </span><span class="nt">deleteEmptyDir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="upgrade-state-machine">
<h3>Upgrade State Machine<a class="headerlink" href="#upgrade-state-machine" title="Permalink to this headline"></a></h3>
<p>The upgrade controller manages driver upgrades through a well-defined state machine.
The node label, <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu-driver-upgrade-state</span></code>, indicates the state a node is currently in.
The set of possible states are:</p>
<ul class="simple">
<li><p>Unknown (empty): The upgrade controller is disabled or the node has not been processed yet.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">upgrade-required</span></code>: NVIDIA driver pod is not up-to-date and requires an upgrade. No actions are performed at this stage.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cordon-required</span></code>: Node will be marked Unschedulable in preparation for the driver upgrade.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">wait-for-jobs-required</span></code>: Node will wait on the completion of a group of pods/jobs before proceeding.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pod-deletion-required</span></code>: Pods allocated GPU will be deleted from the node. If pod deletion fails, node moves to <code class="docutils literal notranslate"><span class="pre">drain-required</span></code>
if drain is enabled in ClusterPolicy.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">drain-required</span></code>: Node will be drained. This state is skipped if all GPU pods are successfully deleted from the node.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pod-restart-required</span></code>: The NVIDIA driver pod running on the node will be restarted and upgraded to the new version.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">validation-required</span></code>: Validation of the new driver deployed on the node is required before proceeding. The GPU Operator
performs validations in the pod named <code class="docutils literal notranslate"><span class="pre">operator-validator</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">uncordon-required</span></code>: Node will be marked Schedulable to complete the upgrade process.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">upgrade-done</span></code>: NVIDIA driver pod is up-to-date and running on the node.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">upgrade-failed</span></code>: A failure occurred during the driver upgrade.</p></li>
</ul>
<p>The complete state machine is depicted in the diagram below.</p>
<a class="reference internal image-reference" href="_images/upgrade-controller-state-machine.png"><img alt="_images/upgrade-controller-state-machine.png" src="_images/upgrade-controller-state-machine.png" style="width: 600px;" /></a>
</section>
<section id="pausing-driver-upgrades">
<h3>Pausing Driver Upgrades<a class="headerlink" href="#pausing-driver-upgrades" title="Permalink to this headline"></a></h3>
<p>If you desire to pause the automatic driver upgrade process in the cluster, toggle <code class="docutils literal notranslate"><span class="pre">driver.upgradePolicy.autoUpgrade</span></code> flag
in ClusterPolicy.
The entire state machine will pause and effectively disable any pending nodes from being upgraded.
This flag can be dynamically toggled again to re-enable the upgrade controller and resume any pending upgrades.</p>
</section>
<section id="skipping-driver-upgrades">
<h3>Skipping Driver Upgrades<a class="headerlink" href="#skipping-driver-upgrades" title="Permalink to this headline"></a></h3>
<p>If you desire to skip driver upgrades on a certain node, label the node with <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu-driver-upgrade.skip=true</span></code>.</p>
</section>
<section id="metrics-and-events">
<h3>Metrics and Events<a class="headerlink" href="#metrics-and-events" title="Permalink to this headline"></a></h3>
<p>The GPU Operator will generate the following metrics during the upgrade process which can be scraped by Prometheus.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">gpu_operator_auto_upgrade_enabled</span></code>: 1 if driver auto upgrade is enabled; 0 if not.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpu_operator_nodes_upgrades_in_progress</span></code>: Total number of nodes in which a driver pod is being upgraded on.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpu_operator_nodes_upgrades_done</span></code>: Total number of nodes in which a driver pod has been successfully upgraded.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpu_operator_nodes_upgrades_failed</span></code>: Total number of nodes in which a driver pod upgrade has failed.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpu_operator_nodes_upgrades_available</span></code>: Total number of nodes in which a driver pod upgrade can start on.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpu_operator_nodes_upgrades_pending</span></code>: Total number of nodes in which driver pod upgrades are pending.</p></li>
</ul>
<p>The GPU Operator will generate events during the upgrade process.
The most common events are for state transitions or failures at a particular state.
Below are an example set of events generated for the upgrade of one node.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get events --sort-by<span class="o">=</span><span class="s1">&#39;.lastTimestamp&#39;</span> <span class="p">|</span> grep GPUDriverUpgrade
<span class="go">10m         Normal   GPUDriverUpgrade     node/localhost.localdomain   Successfully updated node state label to [upgrade-required]</span>
<span class="go">10m         Normal   GPUDriverUpgrade     node/localhost.localdomain   Successfully updated node state label to [cordon-required]</span>
<span class="go">10m         Normal   GPUDriverUpgrade     node/localhost.localdomain   Successfully updated node state label to [wait-for-jobs-required]</span>
<span class="go">10m         Normal   GPUDriverUpgrade     node/localhost.localdomain   Successfully updated node state label to [pod-deletion-required]</span>
<span class="go">10m         Normal   GPUDriverUpgrade     node/localhost.localdomain   Successfully updated node state label to [pod-restart-required]</span>
<span class="go">7m          Normal   GPUDriverUpgrade     node/localhost.localdomain   Successfully updated node state label to [validation-required]</span>
<span class="go">6m          Normal   GPUDriverUpgrade     node/localhost.localdomain   Successfully updated node state label to [uncordon-required]</span>
<span class="go">6m          Normal   GPUDriverUpgrade     node/localhost.localdomain   Successfully updated node state label to [upgrade-done]</span>
</pre></div>
</div>
</section>
<section id="troubleshooting">
<h3>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this headline"></a></h3>
<p>If the upgrade fails for a particular node, the node will be marked in the <code class="docutils literal notranslate"><span class="pre">upgrade-failed</span></code> state.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get node -l nvidia.com/gpu.present <span class="se">\</span>
    -ojsonpath<span class="o">=</span><span class="s1">&#39;{range .items[*]}{.metadata.name}{&quot;\t&quot;}{.metadata.labels.nvidia\.com/gpu-driver-upgrade-state}{&quot;\n&quot;}{end}&#39;</span>
<span class="go">k8s-node-1 upgrade-done</span>
<span class="go">k8s-node-2 upgrade-done</span>
<span class="go">k8s-node-3 upgrade-failed</span>
</pre></div>
</div>
<p>Check events to better understand at which stage the ugprade failed:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get events --sort-by<span class="o">=</span><span class="s1">&#39;.lastTimestamp&#39;</span> <span class="p">|</span> grep GPUDriverUpgrade
</pre></div>
</div>
<p>Optionally, you can check the gpu-operator logs generated for the upgrade controller:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl logs -n gpu-operator gpu-operator-xxxxx <span class="p">|</span> grep controllers.Upgrade
</pre></div>
</div>
<p>After resolving the upgrade failures for a particular node, you can restart the upgrade process on the node by placing it in the <code class="docutils literal notranslate"><span class="pre">upgrade-required</span></code> state:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl label node &lt;node-name&gt;  nvidia.com/gpu-driver-upgrade-state<span class="o">=</span>upgrade-required --overwrite
</pre></div>
</div>
</section>
</section>
<section id="upgrades-without-the-upgrade-controller">
<h2>Upgrades without the Upgrade Controller<a class="headerlink" href="#upgrades-without-the-upgrade-controller" title="Permalink to this headline"></a></h2>
<p>If the upgrade controller is disabled or not supported for your GPU Operator version, a component called <code class="docutils literal notranslate"><span class="pre">k8s-driver-manager</span></code> is responsible
for executing the driver upgrade process.
The <code class="docutils literal notranslate"><span class="pre">k8s-driver-manager</span></code> is an <cite>initContainer</cite> within the driver Daemonset, which ensures all existing GPU driver clients are disabled before
unloading the current driver modules and continuing with the new driver installation.
This method still automates the core driver upgrade process, but lacks the observability that the upgrade controller provides as well as additional
controls such as pausing/skipping upgrades.
In addition, no new features will be added to the <code class="docutils literal notranslate"><span class="pre">k8s-driver-manager</span></code> moving forward in favor of the upgrade controller.</p>
<p class="rubric">Procedure</p>
<ol class="arabic">
<li><p>Upgrade the driver by changing <code class="docutils literal notranslate"><span class="pre">driver.version</span></code> value in ClusterPolicy:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl patch clusterpolicy/cluster-policy --type<span class="o">=</span><span class="s1">&#39;json&#39;</span> -p<span class="o">=</span><span class="s1">&#39;[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/driver/version&quot;, &quot;value&quot;:&quot;510.85.02&quot;}]&#39;</span>
</pre></div>
</div>
</li>
<li><p>(Optional) To monitor the status of the upgrade, watch the deployment of the new driver pod on GPU worker nodes:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kubectl get pods -n gpu-operator -lapp=nvidia-driver-daemonset -w</span>
</pre></div>
</div>
</li>
</ol>
<section id="id2">
<h3>Configuration Options<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h3>
<p>The following configuration options are available for <code class="docutils literal notranslate"><span class="pre">k8s-driver-manager</span></code>. The options allow users to control the
GPU pod eviction / node drain behavior.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">driver</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">manager</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">env</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ENABLE_GPU_POD_EVICTION</span><span class="w"></span>
<span class="w">      </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ENABLE_AUTO_DRAIN</span><span class="w"></span>
<span class="w">      </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">DRAIN_USE_FORCE</span><span class="w"></span>
<span class="w">      </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;false&quot;</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">DRAIN_POD_SELECTOR_LABEL</span><span class="w"></span>
<span class="w">      </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">DRAIN_TIMEOUT_SECONDS</span><span class="w"></span>
<span class="w">      </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;0s&quot;</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">DRAIN_DELETE_EMPTYDIR_DATA</span><span class="w"></span>
<span class="w">      </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;false&quot;</span><span class="w"></span>
</pre></div>
</div>
<ul class="simple">
<li><p>The <em>ENABLE_GPU_POD_EVICTION</em> env allows <code class="docutils literal notranslate"><span class="pre">k8s-driver-manager</span></code> to attempt evicting only GPU pods from the node before attempting a node drain. Only if this fails and
<em>ENABLE_AUTO_DRAIN</em> is enabled will the node ever be drained.</p></li>
<li><p>The <em>DRAIN_USE_FORCE</em> env needs to be enabled for evicting GPU pods that are not managed by any of the replication controllers (Deployment, Daemonset, StatefulSet, ReplicaSet).</p></li>
<li><p>The <em>DRAIN_DELETE_EMPTYDIR_DATA</em> env needs to be enabled to allow deletion of GPU pods using emptyDir volume.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<blockquote>
<div><p>Since GPU pods get evicted whenever the NVIDIA Driver Daemonset spec is updated, it may not always be desirable to allow this to happen automatically.
To prevent this <code class="docutils literal notranslate"><span class="pre">daemonsets.updateStrategy</span></code> parameter in the <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code> can be set to <a class="reference external" href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#daemonset-update-strategy">OnDelete</a> .
With <code class="docutils literal notranslate"><span class="pre">OnDelete</span></code> update strategy, a new driver pod with the updated spec will only get deployed on a node once the old driver pod is manually deleted.</p>
</div></blockquote>
<p>Thus, admins can control when to rollout spec updates to driver pods on any given node.
For more information on DaemonSet update strategies, refer to the <a class="reference external" href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#daemonset-update-strategy">Kubernetes documentation</a>.</p>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2023, NVIDIA.
      <span class="lastupdated">Last updated on Mar 02, 2023.
      </span></p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>