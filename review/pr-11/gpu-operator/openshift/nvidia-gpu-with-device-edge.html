<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Accelerating workloads with NVIDIA GPUs with Red Hat Device Edge &mdash; NVIDIA Cloud Native Technologies  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../../_static/nvidia.ico"/>
    <link rel="canonical" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/nvidia-gpu-with-device-edge.html"/>
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
        <script src="../../_static/js/google-analytics/google-analytics-tracker.js"></script>
        <script src="../../_static/js/google-analytics/google-analytics-write.js"></script>
        <script src="//assets.adobedtm.com/b92787824f2e0e9b68dc2e993f9bd995339fe417/satelliteLib-7ba51e58dc61bcb0e9311aadd02a0108ab24cc6c.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="DCGM-Exporter" href="../../gpu-telemetry/dcgm-exporter.html" />
    <link rel="prev" title="Appendix" href="appendix-ocp.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" > 
            <a href="../../contents.html">
            <img src="../../_static/NVLogo_H_B&W.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }

    .wy-nav-content {
      max-width: 1000px;
    }
  </style>
  
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">NVIDIA Container Toolkit:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../container-toolkit/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../container-toolkit/concepts.html">Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../container-toolkit/arch-overview.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../container-toolkit/install-guide.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../container-toolkit/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../container-toolkit/user-guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../container-toolkit/release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../container-toolkit/archive.html">Archive</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">NVIDIA GPU Operator:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platform-support.html">Platform Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-driver-upgrades.html">GPU Driver Upgrades</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install-gpu-operator-vgpu.html">NVIDIA vGPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install-gpu-operator-nvaie.html">NVIDIA AI Enterprise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator-mig.html">GPU Operator with MIG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-sharing.html">Time-Slicing GPUs in Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator-rdma.html">GPUDirect RDMA and GPUDirect Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator-kubevirt.html">GPU Operator with KubeVirt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix.html">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../archive.html">Archive</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Kubernetes with GPUs:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../kubernetes/install-k8s.html">Install Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../kubernetes/mig-k8s.html">MIG Support in Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../kubernetes/anthos-guide.html">NVIDIA GPUs with Google Cloud’s Anthos</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">NVIDIA GPUs and Red Hat OpenShift</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="prerequisites.html">Prerequisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="steps-overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-nfd.html">Installing the Node Feature Discovery (NFD) Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-ocp.html">Installing the NVIDIA GPU Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="nvaie-with-ocp.html">NVIDIA AI Enterprise with OpenShift</a></li>
<li class="toctree-l1"><a class="reference internal" href="mig-ocp.html">MIG Support in OpenShift Container Platform</a></li>
<li class="toctree-l1"><a class="reference internal" href="clean-up.html">Cleanup</a></li>
<li class="toctree-l1"><a class="reference internal" href="mirror-gpu-ocp-disconnected.html">Deploy GPU Operators in a disconnected or airgapped environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable-gpu-op-dashboard.html">Enable the GPU Operator Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="time-slicing-gpus-in-openshift.html">Time-slicing NVIDIA GPUs in OpenShift</a></li>
<li class="toctree-l1"><a class="reference internal" href="openshift-virtualization.html">NVIDIA GPU Operator with OpenShift Virtualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="troubleshooting-gpu-ocp.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix-ocp.html">Appendix</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">NVIDIA GPUs and Red Hat Device Edge</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Accelerating workloads with NVIDIA GPUs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Telemetry:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-telemetry/dcgm-exporter.html">DCGM-Exporter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-telemetry/dcgm-exporter.html#integrating-gpu-telemetry-into-kubernetes">Integrating GPU Telemetry into Kubernetes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multi-Instance GPU:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../mig/mig.html">Multi-Instance GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mig/mig-k8s.html">MIG Support in Kubernetes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Driver Containers:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../driver-containers/overview.html">Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Playground:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../playground/dind.html">Docker-in-Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playground/x-arch.html">Running Cross-Architecture Containers</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../contents.html">NVIDIA Cloud Native Technologies</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../contents.html" class="icon icon-home"></a> &raquo;</li>
      <li>Accelerating workloads with NVIDIA GPUs with Red Hat Device Edge</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="accelerating-workloads-with-nvidia-gpus-with-red-hat-device-edge">
<span id="mirror-gpu-ocp-disconnected"></span><h1>Accelerating workloads with NVIDIA GPUs with Red Hat Device Edge<a class="headerlink" href="#accelerating-workloads-with-nvidia-gpus-with-red-hat-device-edge" title="Permalink to this heading"></a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#introduction" id="id1">Introduction</a></p></li>
<li><p><a class="reference internal" href="#prerequisites" id="id2">Prerequisites</a></p></li>
<li><p><a class="reference internal" href="#installing-the-nvidia-gpu-driver" id="id3">Installing the NVIDIA GPU driver</a></p></li>
<li><p><a class="reference internal" href="#installing-the-nvidia-container-toolkit" id="id4">Installing the NVIDIA Container Toolkit</a></p></li>
<li><p><a class="reference internal" href="#installing-the-nvidia-device-plugin" id="id5">Installing the NVIDIA Device Plugin</a></p></li>
<li><p><a class="reference internal" href="#running-a-gpu-accelerated-workload-on-red-hat-device-edge" id="id6">Running a GPU-accelerated workload on Red Hat Device Edge</a></p></li>
</ul>
</div>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading"></a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Developer Preview features are not supported with Red Hat production service level agreements (SLAs) and are not functionally complete. Red Hat does not advise using them in a production setting. Developer Preview features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process. These releases may not have any documentation, and testing is limited. Red Hat may provide ways to submit feedback on Developer Preview releases without an associated SLA.</p>
</div>
<p>Red Hat has released Red Hat Device Edge, which provides access to MicroShift. MicroShift offers the simplicity of single-node deployment with the functionality and services you need for computing in resource-constrained locations. You can have many deployments on different hosts, creating the specific system image needed for each of your applications. Installing MicroShift on top of your managed RHEL devices in hard-to-service locations also allows for streamlined over-the-air updates.</p>
<p>Red Hat Device Edge combines light-weight Kubernetes using MicroShift with Red Hat Enterprise Linux at the edge. MicroShift is a Kubernetes implementation derived from OpenShift, focusing on a minimal  footprint. Red Hat Device Edge addresses the needs of bare metal, virtual, containerized, or kubernetes workloads deployed to resource constrained environments.</p>
<p>The procedures described on this page tell you how to enable NVIDIA GPUs on an x86 system
running Red Hat Device Edge.</p>
</section>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this heading"></a></h2>
<ul>
<li><p>Install <a class="reference external" href="https://access.redhat.com/documentation/en-us/red_hat_build_of_microshift/4.12/html/installing/microshift-install-rpm?extIdCarryOver=true&amp;sc_cid=701f2000001Css5AAC">Microshift from an RPM package</a> on the Red Hat Enterprise Linux 8.7 machine.</p></li>
<li><p>Verify an NVIDIA GPU is installed on the machine by running the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>lspci<span class="w"> </span>-nnv<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>-i<span class="w"> </span>nvidia
</pre></div>
</div>
<p><strong>Example output</strong></p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">17:00.0 3D controller [0302]: NVIDIA Corporation GA100GL [A30 PCIe] [10de:20b7] (rev a1)</span>
<span class="go">        Subsystem: NVIDIA Corporation Device [10de:1532]</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="installing-the-nvidia-gpu-driver">
<h2>Installing the NVIDIA GPU driver<a class="headerlink" href="#installing-the-nvidia-gpu-driver" title="Permalink to this heading"></a></h2>
<p>NVIDIA provides a precompiled driver in RPM repositories that implement the modularity mechanism.
For more information, see <a class="reference external" href="https://developer.nvidia.com/blog/streamlining-nvidia-driver-deployment-on-rhel-8-with-modularity-streams/">Streamlining NVIDIA Driver Deployment on RHEL 8 with Modularity Streams</a>.</p>
<p>#. At this stage, you should have already subscribed your machine and enabled the <code class="docutils literal notranslate"><span class="pre">rhel-8-for-x86_64-baseos-rpms</span></code> and <code class="docutils literal notranslate"><span class="pre">rhel-8-for-x86_64-appstream-rpms</span></code> repositories.
Add the NVIDIA repository by running the following command:</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>dnf<span class="w"> </span>config-manager<span class="w"> </span>--add-repo<span class="o">=</span>https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo
</pre></div>
</div>
</div></blockquote>
<p>#. NVIDIA provides different branches of their drivers, with different lifecycles, that are described in <a class="reference external" href="https://docs.nvidia.com/datacenter/tesla/drivers/index.html#cuda-drivers">NVIDIA Datacenter Drivers documentation</a>.
Use the latest version from the production branch, for example,version <code class="docutils literal notranslate"><span class="pre">R525</span></code>. Install the driver by running the following command:</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>dnf<span class="w"> </span>module<span class="w"> </span>install<span class="w"> </span>nvidia-driver:525
</pre></div>
</div>
</div></blockquote>
<ol class="arabic">
<li><p>After installing the driver, you need to disable the <code class="docutils literal notranslate"><span class="pre">nouveau</span></code> driver because it will conflict with the NVIDIA driver:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">echo</span><span class="w"> </span><span class="s1">&#39;blacklist nouveau&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>sudo<span class="w"> </span>tee<span class="w"> </span>/etc/modprobe.d/nouveau-blacklist.conf
</pre></div>
</div>
</li>
<li><p>Reboot the machine:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>systemctl<span class="w"> </span>reboot
</pre></div>
</div>
</li>
<li><p>After the machine has rebooted, verify that the NVIDIA drivers are installed properly:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>nvidia-smi
</pre></div>
</div>
<p><strong>Example output</strong></p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">Fri Jan 13 14:29:53 2023</span>
<span class="go">+-----------------------------------------------------------------------------+</span>
<span class="go">| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |</span>
<span class="go">|-------------------------------+----------------------+----------------------+</span>
<span class="go">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span>
<span class="go">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span>
<span class="go">|                               |                      |               MIG M. |</span>
<span class="go">|===============================+======================+======================|</span>
<span class="go">|   0  NVIDIA A30          Off  | 00000000:17:00.0 Off |                    0 |</span>
<span class="go">| N/A   29C    P0    35W / 165W |      0MiB / 24576MiB |     25%      Default |</span>
<span class="go">|                               |                      |             Disabled |</span>
<span class="go">+-------------------------------+----------------------+----------------------+</span>

<span class="go">+-----------------------------------------------------------------------------+</span>
<span class="go">| Processes:                                                                  |</span>
<span class="go">|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |</span>
<span class="go">|        ID   ID                                                   Usage      |</span>
<span class="go">|=============================================================================|</span>
<span class="go">|  No running processes found                                                 |</span>
<span class="go">+-----------------------------------------------------------------------------+</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="installing-the-nvidia-container-toolkit">
<h2>Installing the NVIDIA Container Toolkit<a class="headerlink" href="#installing-the-nvidia-container-toolkit" title="Permalink to this heading"></a></h2>
<p>The <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html">NVIDIA Container Toolkit</a> allows users
to build and run GPU accelerated containers. The toolkit includes a container runtime library and utilities to automatically configure containers
to leverage NVIDIA GPUs. You have to install it to allow the container runtime to transparently configure the NVIDIA GPUs for the pods deployed in Microshift.</p>
<p>The NVIDIA container toolkit supports the distributions listed in the <a class="reference external" href="https://nvidia.github.io/nvidia-docker/">NVIDIA Container Toolkit repository</a>.</p>
<ol class="arabic">
<li><p>Install the NVIDIA Container Toolkit for RHEL 8.6 by running the following commands:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>curl<span class="w"> </span>-s<span class="w"> </span>-L<span class="w"> </span>https://nvidia.github.io/libnvidia-container/rhel8.7/libnvidia-container.repo<span class="w"> </span><span class="p">|</span><span class="w"> </span>sudo<span class="w"> </span>tee<span class="w"> </span>/etc/yum.repos.d/libnvidia-container.repo
<span class="gp">$ </span>sudo<span class="w"> </span>dnf<span class="w"> </span>install<span class="w"> </span>nvidia-container-toolkit<span class="w"> </span>-y
</pre></div>
</div>
</li>
<li><p>The NVIDIA Container Toolkit requires some SELinux permissions to work properly. These permissions are set in three steps.</p>
<ol class="arabic">
<li><p>Allow containers to use devices from the host by running the following commands:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>dnf<span class="w"> </span>install<span class="w"> </span>container-selinux.noarch
<span class="gp">$ </span>sudo<span class="w"> </span>setsebool<span class="w"> </span>-P<span class="w"> </span>container_use_devices<span class="w"> </span>on
</pre></div>
</div>
</li>
<li><p>It is still missing a permission, so create a policy file:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat<span class="w"> </span>&lt;&lt;EOF<span class="w"> </span>&gt;<span class="w"> </span>nvidia-container-microshift.te
<span class="go">module nvidia-container-microshift 1.0;</span>

<span class="go">require {</span>
<span class="go">              type xserver_misc_device_t;</span>
<span class="go">              type container_t;</span>
<span class="go">              class chr_file { map read write };</span>
<span class="go">}</span>

<span class="gp">#</span><span class="o">=============</span><span class="w"> </span><span class="nv">container_t</span><span class="w"> </span><span class="o">==============</span>
<span class="go">allow container_t xserver_misc_device_t:chr_file map;</span>
<span class="go">EOF</span>
</pre></div>
</div>
</li>
<li><p>Compile and apply the policy by running the following commands:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>checkmodule<span class="w"> </span>-m<span class="w"> </span>-M<span class="w"> </span>-o<span class="w"> </span>nvidia-container-microshift.mod<span class="w"> </span>nvidia-container-microshift.te
<span class="gp">$ </span>semodule_package<span class="w"> </span>--outfile<span class="w"> </span>nvidia-container-microshift.pp<span class="w"> </span>--module<span class="w"> </span>nvidia-container-microshift.mod
<span class="gp">$ </span>sudo<span class="w"> </span>semodule<span class="w"> </span>-i<span class="w"> </span>nvidia-container-microshift.pp
</pre></div>
</div>
</li>
</ol>
</li>
</ol>
</section>
<section id="installing-the-nvidia-device-plugin">
<h2>Installing the NVIDIA Device Plugin<a class="headerlink" href="#installing-the-nvidia-device-plugin" title="Permalink to this heading"></a></h2>
<p>In order for Microshift to be able to allocate GPU resource to the pods, you need to
deploy the <a class="reference external" href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA Device Plugin</a>, which is the Daemonset that allows you to automatically:</p>
<ul class="simple">
<li><p>Expose the number of GPUs on each nodes of your cluster</p></li>
<li><p>Keep track of the health of your GPUs</p></li>
<li><p>Run GPU-enabled containers in your Kubernetes cluster</p></li>
</ul>
<p>The deployment consists of adding manifests and a <code class="docutils literal notranslate"><span class="pre">kustomize</span></code> configuration to the <code class="docutils literal notranslate"><span class="pre">/etc/microshift/manifests</span></code> folder where Microshift looks for manifests to create at start time. This is explained in the <a class="reference external" href="https://access.redhat.com/documentation/en-us/red_hat_build_of_microshift/4.12/html/configuring/index">Configuring section of the Microshift documentation</a>.</p>
<ol class="arabic">
<li><p>Create the <cite>manifests</cite> folder by running the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/etc/microshift/manifests
</pre></div>
</div>
</li>
<li><p>The device plugin runs in privileged mode, so you need to isolate it from other workloads by running it in its own namespace, <code class="docutils literal notranslate"><span class="pre">nvidia-device-plugin</span></code>. To add the plugin to the manifests deployed by Microshift at start time, we download it as <code class="docutils literal notranslate"><span class="pre">/etc/microshift/manifests/nvidia-device-plugin.yml</span></code>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>curl<span class="w"> </span>-s<span class="w"> </span>-L<span class="w"> </span>https://gitlab.com/nvidia/kubernetes/device-plugin/-/raw/main/deployments/static/nvidia-device-plugin-privileged-with-service-account.yml<span class="w"> </span><span class="p">|</span><span class="w"> </span>sudo<span class="w"> </span>tee<span class="w"> </span>/etc/microshift/manifests/nvidia-device-plugin.yml
</pre></div>
</div>
</li>
<li><p>The resources are not created automatically even though the files exist. You need to add them to the <code class="docutils literal notranslate"><span class="pre">kustomize</span></code> configuration. Do this by adding a single <code class="docutils literal notranslate"><span class="pre">kustomization.yaml</span></code> file in the <code class="docutils literal notranslate"><span class="pre">manifests</span></code> folder that references all the resources you want to create.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat<span class="w"> </span>&lt;&lt;EOF<span class="w"> </span><span class="p">|</span><span class="w"> </span>sudo<span class="w"> </span>tee<span class="w"> </span>/etc/microshift/manifests/kustomization.yaml
<span class="go">---</span>
<span class="go">apiVersion: kustomize.config.k8s.io/v1beta1</span>
<span class="go">kind: Kustomization</span>
<span class="go">resources:</span>
<span class="go">  - nvidia-device-plugin.yml</span>
<span class="go">EOF</span>
</pre></div>
</div>
</li>
<li><p>Restart the Microshift service so that it creates the resources by running the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>microshift
</pre></div>
</div>
</li>
<li><p>When Microshift has restarted, verify that the pod is running in the <code class="docutils literal notranslate"><span class="pre">nvidia-device-plugin</span></code> namespace by running the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc<span class="w"> </span>get<span class="w"> </span>pod<span class="w"> </span>-n<span class="w"> </span>nvidia-device-plugin
</pre></div>
</div>
<p><strong>Example output</strong></p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">NAMESPACE                  NAME                                   READY   STATUS        RESTARTS     AGE</span>
<span class="go">nvidia-device-plugin       nvidia-device-plugin-daemonset-jx8s8   1/1     Running       0            1m</span>
</pre></div>
</div>
</li>
</ol>
<p>.# Verify in the the log that it has registered itself as a device plugin for the <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu</span></code> resources.</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc<span class="w"> </span>logs<span class="w"> </span>-n<span class="w"> </span>nvidia-device-plugin<span class="w"> </span>nvidia-device-plugin-jx8s8
</pre></div>
</div>
<p><strong>Example output</strong></p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">[...]</span>
<span class="go">2022/12/13 04:17:38 Retreiving plugins.</span>
<span class="go">2022/12/13 04:17:38 Detected NVML platform: found NVML library</span>
<span class="go">2022/12/13 04:17:38 Detected non-Tegra platform: /sys/devices/soc0/family file not found</span>
<span class="go">2022/12/13 04:17:38 Starting GRPC server for &#39;nvidia.com/gpu&#39;</span>
<span class="go">2022/12/13 04:17:38 Starting to serve &#39;nvidia.com/gpu&#39; on /var/lib/kubelet/device-plugins/nvidia-gpu.sock</span>
<span class="go">2022/12/13 04:17:38 Registered device plugin for &#39;nvidia.com/gpu&#39; with Kubelet</span>
</pre></div>
</div>
</div></blockquote>
<ol class="arabic">
<li><p>You can also verify that the node exposes the <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu</span></code> resources in its capacity.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc<span class="w"> </span>get<span class="w"> </span>node<span class="w"> </span>-o<span class="w"> </span>json<span class="w"> </span><span class="p">|</span><span class="w"> </span>jq<span class="w"> </span>-r<span class="w"> </span><span class="s1">&#39;.items[0].status.capacity&#39;</span>
</pre></div>
</div>
<p><strong>Example output</strong></p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">  &quot;cpu&quot;: &quot;48&quot;,</span>
<span class="go">  &quot;ephemeral-storage&quot;: &quot;142063152Ki&quot;,</span>
<span class="go">  &quot;hugepages-1Gi&quot;: &quot;0&quot;,</span>
<span class="go">  &quot;hugepages-2Mi&quot;: &quot;0&quot;,</span>
<span class="go">  &quot;memory&quot;: &quot;196686216Ki&quot;,</span>
<span class="go">  &quot;nvidia.com/gpu&quot;: &quot;1&quot;,</span>
<span class="go">  &quot;pods&quot;: &quot;250&quot;</span>
<span class="go">}</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="running-a-gpu-accelerated-workload-on-red-hat-device-edge">
<h2>Running a GPU-accelerated workload on Red Hat Device Edge<a class="headerlink" href="#running-a-gpu-accelerated-workload-on-red-hat-device-edge" title="Permalink to this heading"></a></h2>
<p>When the device plugin is running, you can run workloads that leverage the acceleration.
A simple test is using the CUDA vectorAdd program, which is provided by NVIDIA as a container image,
so it’s easy to use.</p>
<ol class="arabic">
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">test</span></code> namespace by running the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc<span class="w"> </span>create<span class="w"> </span>namespace<span class="w"> </span><span class="nb">test</span>
</pre></div>
</div>
</li>
<li><p>Define the pod specification. Note the <code class="docutils literal notranslate"><span class="pre">spec.containers[0].resources.limits</span></code> field where the <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu</span></code> resource specifies a value of <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat<span class="w"> </span>&lt;&lt;<span class="w"> </span>EOF<span class="w"> </span>&gt;<span class="w"> </span>pod-cuda-vector-add.yaml
<span class="go">---</span>
<span class="go">apiVersion: v1</span>
<span class="go">kind: Pod</span>
<span class="go">metadata:</span>
<span class="go">  name: test-cuda-vector-add</span>
<span class="go">  namespace: test</span>
<span class="go">spec:</span>
<span class="go">  restartPolicy: OnFailure</span>
<span class="go">  containers:</span>
<span class="go">  - name: cuda-vector-add</span>
<span class="go">    image: &quot;nvidia/samples:vectoradd-cuda11.2.1-ubi8&quot;</span>
<span class="go">    resources:</span>
<span class="go">      limits:</span>
<span class="go">        nvidia.com/gpu: 1</span>
<span class="go">    securityContext:</span>
<span class="go">      allowPrivilegeEscalation: false</span>
<span class="go">      capabilities:</span>
<span class="go">        drop: [&quot;ALL&quot;]</span>
<span class="go">      runAsNonRoot: true</span>
<span class="go">      seccompProfile:</span>
<span class="go">        type: &quot;RuntimeDefault&quot;</span>
<span class="go">EOF</span>
</pre></div>
</div>
</li>
<li><p>Create the pod by running the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>pod-cuda-vector-add.yaml
</pre></div>
</div>
</li>
<li><p>Verify the pod log has found a CUDA device by running the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc<span class="w"> </span>logs<span class="w"> </span>-n<span class="w"> </span><span class="nb">test</span><span class="w"> </span>test-cuda-vector-add
</pre></div>
</div>
<p><strong>Example output</strong></p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">[Vector addition of 50000 elements]</span>
<span class="go">Copy input data from the host memory to the CUDA device</span>
<span class="go">CUDA kernel launch with 196 blocks of 256 threads</span>
<span class="go">Copy output data from the CUDA device to the host memory</span>
<span class="go">Test PASSED</span>
<span class="go">Done</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="appendix-ocp.html" class="btn btn-neutral float-left" title="Appendix" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../gpu-telemetry/dcgm-exporter.html" class="btn btn-neutral float-right" title="DCGM-Exporter" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018-2023, NVIDIA Corporation.
      <span class="lastupdated">Last updated on 2023-03-14.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>  

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>